{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/besherh/BigDataManagement/blob/main/SparkNotebooks/PySpark_Trasformation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00000-a579b3bd-6eb6-4445-8bb9-82fb1dc1e072",
        "deepnote_cell_type": "markdown",
        "id": "21NeyoHgLnER"
      },
      "source": [
        "# PySpark\n",
        "\n",
        "![Logo](https://github.com/pnavaro/big-data/blob/master/notebooks/images/apache_spark_logo.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00001-bf31e048-ace9-4993-8da2-5f1af4015c57",
        "deepnote_cell_type": "markdown",
        "id": "N-gq14MILnEX"
      },
      "source": [
        "- [Apache Spark](https://spark.apache.org) was first released in 2014. \n",
        "- It was originally developed by [Matei Zaharia](http://people.csail.mit.edu/matei) as a class project, and later a PhD dissertation, at University of California, Berkeley.\n",
        "- Spark is written in [Scala](https://www.scala-lang.org).\n",
        "- All images come from [Databricks](https://databricks.com/product/getting-started-guide)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00002-bb8dce6d-d8e9-486b-818d-f6dcc250b446",
        "deepnote_cell_type": "markdown",
        "id": "EI0eAfCMLnEY"
      },
      "source": [
        "- Apache Spark is a fast and general-purpose cluster computing system. \n",
        "- It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs.\n",
        "- Spark can manage \"big data\" collections with a small set of high-level primitives like `map`, `filter`, `groupby`, and `join`.  With these common patterns we can often handle computations that are more complex than map, but are still structured.\n",
        "- It also supports a rich set of higher-level tools including [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) for SQL and structured data processing, [MLlib](https://spark.apache.org/docs/latest/ml-guide.html) for machine learning, [GraphX](https://spark.apache.org/docs/latest/graphx-programming-guide.html) for graph processing, and Spark Streaming."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00003-d67dd6f1-d704-4caa-81cf-5a9071b62f17",
        "deepnote_cell_type": "markdown",
        "id": "Hq5N1QkLLnEZ"
      },
      "source": [
        "## Resilient distributed datasets\n",
        "\n",
        "- The fundamental abstraction of Apache Spark is a read-only, parallel, distributed, fault-tolerent collection called a resilient distributed datasets (RDD).\n",
        "- RDDs behave a bit like Python collections (e.g. lists).\n",
        "- When working with Apache Spark we iteratively apply functions to every item of these collections in parallel to produce *new* RDDs.\n",
        "- The data is distributed across nodes in a cluster of computers.\n",
        "- Functions implemented in Spark can work in parallel across elements of the collection.\n",
        "- The  Spark framework allocates data and processing to different nodes, without any intervention from the programmer.\n",
        "- RDDs automatically rebuilt on machine failure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00004-befe75de-d238-405e-81e7-38cedbf082ce",
        "deepnote_cell_type": "markdown",
        "id": "BZG0tAFpLnEZ"
      },
      "source": [
        "## Lifecycle of a Spark Program\n",
        "\n",
        "1. Create some input RDDs from external data or parallelize a collection in your driver program.\n",
        "2. Lazily transform them to define new RDDs using transformations like `filter()` or `map()`\n",
        "3. Ask Spark to cache() any intermediate RDDs that will need to be reused.\n",
        "4. Launch actions such as count() and collect() to kick off a parallel computation, which is then optimized and executed by Spark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00005-7b21b99e-f856-4c8e-9be8-3c9e9f4e9630",
        "deepnote_cell_type": "markdown",
        "id": "qZnh4i6hLnEa"
      },
      "source": [
        "## Operations on Distributed Data\n",
        "\n",
        "- Two types of operations: **transformations** and **actions**\n",
        "- Transformations are *lazy* (not computed immediately) \n",
        "- Transformations are executed when an action is run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00006-d114d429-bb93-4c93-affc-216589f7db5e",
        "deepnote_cell_type": "markdown",
        "id": "dsw5t5YdLnEa"
      },
      "source": [
        "## [Transformations](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations) (lazy)\n",
        "\n",
        "```\n",
        "map() flatMap()\n",
        "filter() \n",
        "mapPartitions() mapPartitionsWithIndex() \n",
        "sample()\n",
        "union() intersection() distinct()\n",
        "groupBy() groupByKey()\n",
        "reduceBy() reduceByKey()\n",
        "sortBy() sortByKey()\n",
        "join()\n",
        "cogroup()\n",
        "cartesian()\n",
        "pipe()\n",
        "coalesce()\n",
        "repartition()\n",
        "partitionBy()\n",
        "...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00007-c19d8245-7c9f-407b-8518-0cdbb557be79",
        "deepnote_cell_type": "markdown",
        "id": "YSU9f-QnLnEc"
      },
      "source": [
        "## [Actions](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions)\n",
        "\n",
        "```\n",
        "reduce()\n",
        "collect()\n",
        "count()\n",
        "first()\n",
        "take()\n",
        "takeSample()\n",
        "saveToCassandra()\n",
        "takeOrdered()\n",
        "saveAsTextFile()\n",
        "saveAsSequenceFile()\n",
        "saveAsObjectFile()\n",
        "countByKey()\n",
        "foreach()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00008-385816d2-d5dd-4556-a0ec-2bbc0d0f54dc",
        "deepnote_cell_type": "markdown",
        "id": "oMjZ42FtLnEc"
      },
      "source": [
        "## Python API\n",
        "\n",
        "PySpark uses Py4J that enables Python programs to dynamically access Java objects.\n",
        "\n",
        "![PySpark Internals](https://github.com/pnavaro/big-data/blob/master/notebooks/images/YlI8AqEl.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00009-f66ff6fd-edde-4c4c-bfa1-ba0d1d96ba97",
        "deepnote_cell_type": "markdown",
        "id": "mfuQBCYcLnEd"
      },
      "source": [
        "## The `SparkContext` class\n",
        "\n",
        "- When working with Apache Spark we invoke methods on an object which is an instance of the `pyspark.SparkContext` context.\n",
        "\n",
        "- Typically, an instance of this object will be created automatically for you and assigned to the variable `sc`.\n",
        "\n",
        "- The `parallelize` method in `SparkContext` can be used to turn any ordinary Python collection into an RDD;\n",
        "    - normally we would create an RDD from a large file or an HBase table."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00010-a1e30157-0efd-490d-8b63-459dfb96da8a",
        "deepnote_cell_type": "markdown",
        "id": "ODKP4wTVLnEe"
      },
      "source": [
        "## First example\n",
        "\n",
        "PySpark isn't on sys.path by default, but that doesn't mean it can't be used as a regular library. You can address this by either symlinking pyspark into your site-packages, or adding pyspark to sys.path at runtime. [findspark](https://github.com/minrk/findspark) does the latter.\n",
        "\n",
        "We have a spark context sc to use with a tiny local spark cluster with 4 nodes (will work just fine on a multicore machine)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cell_id": "00012-8698152c-385e-4d22-ae56-408429b08600",
        "deepnote_cell_type": "code",
        "execution_millis": 4,
        "execution_start": 1606208122779,
        "output_cleared": false,
        "source_hash": "a22d9657",
        "id": "igOr9f7DLnEe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "72ed0973-a14e-41f9-c963-587933ba4489"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/usr/bin/python3'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import os, sys\n",
        "sys.executable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cell_id": "00013-7e179656-f878-4ef7-bfc5-4683fd92851d",
        "deepnote_cell_type": "code",
        "execution_millis": 1,
        "execution_start": 1606208260172,
        "output_cleared": false,
        "source_hash": "a04577ae",
        "id": "sTP-p2yLLnEf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bf3a4ee-0cc6-435d-a7db-e14b5285ecae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-470\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 2 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 36.5 MB of archives.\n",
            "After this operation, 143 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jre-headless amd64 8u312-b07-0ubuntu1~18.04 [28.2 MB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jdk-headless amd64 8u312-b07-0ubuntu1~18.04 [8,298 kB]\n",
            "Fetched 36.5 MB in 2s (17.4 MB/s)\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "(Reading database ... 155320 files and directories currently installed.)\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u312-b07-0ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u312-b07-0ubuntu1~18.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u312-b07-0ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u312-b07-0ubuntu1~18.04) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u312-b07-0ubuntu1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u312-b07-0ubuntu1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n"
          ]
        }
      ],
      "source": [
        "!apt-get install openjdk-8-jdk-headless\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop2.7.tgz\n",
        "!tar xf /content/spark-3.2.1-bin-hadoop2.7.tgz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1P1sQXXKYg3n",
        "outputId": "3eff9c76-0512-4e74-906b-56df19839969"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-27 22:53:54--  https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop2.7.tgz\n",
            "Resolving archive.apache.org (archive.apache.org)... 138.201.131.134, 2a01:4f8:172:2ec5::2\n",
            "Connecting to archive.apache.org (archive.apache.org)|138.201.131.134|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 272637746 (260M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.2.1-bin-hadoop2.7.tgz’\n",
            "\n",
            "spark-3.2.1-bin-had 100%[===================>] 260.01M  28.9MB/s    in 9.7s    \n",
            "\n",
            "2022-02-27 22:54:04 (26.8 MB/s) - ‘spark-3.2.1-bin-hadoop2.7.tgz’ saved [272637746/272637746]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark\n",
        "\n"
      ],
      "metadata": {
        "id": "Jxg8NtSuYrJq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop2.7\""
      ],
      "metadata": {
        "id": "IrBkEr9DYugt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7fhB01nFY2V1",
        "outputId": "40cd08c0-9ace-43c5-da7f-924d68ea2100"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/spark-3.2.1-bin-hadoop2.7'"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cell_id": "00014-f661aeba-0a4e-489c-93b7-034ee40d1ef4",
        "deepnote_cell_type": "code",
        "execution_millis": 7637,
        "execution_start": 1606208269332,
        "output_cleared": false,
        "source_hash": "73659aa7",
        "id": "4v7-8pNBLnEg"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "\n",
        "sc = pyspark.SparkContext(master=\"local[*]\", appName=\"FirstExample\")\n",
        "sc.setLogLevel(\"ERROR\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cell_id": "00015-bf748379-3532-453e-8058-f4fc6531a830",
        "deepnote_cell_type": "code",
        "execution_millis": 5,
        "execution_start": 1606208288902,
        "output_cleared": false,
        "source_hash": "3831e90d",
        "id": "nX2S_XffLnEg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e83ad71-abc9-4f0c-9e36-91e023ad6774"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<SparkContext master=local[*] appName=FirstExample>\n"
          ]
        }
      ],
      "source": [
        "print(sc) # it is like a Pool Processor executor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00016-192e38d9-530b-4b44-82f2-0e98ab5d8379",
        "deepnote_cell_type": "markdown",
        "id": "diOwsNZoLnEg"
      },
      "source": [
        "## Create your first RDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "cell_id": "00017-6079893b-b1de-4a98-91ee-cbc10b34cb56",
        "deepnote_cell_type": "code",
        "execution_millis": 549,
        "execution_start": 1606208314201,
        "output_cleared": false,
        "source_hash": "27883792",
        "id": "-Af2LDKyLnEh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c1a2256-c0a6-4e3b-e671-e8b7ce828c40"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "data = list(range(8))\n",
        "rdd = sc.parallelize(data) # create collection\n",
        "rdd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00018-3d4cb001-a63e-4efa-a375-656a819b62f7",
        "deepnote_cell_type": "markdown",
        "id": "eFt6jw9sLnEh"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "Create a file `sample.txt`with lorem package. Read and load it into a RDD with the `textFile` spark function."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#you could read more about Faker here\n",
        "#https://faker.readthedocs.io/en/master/providers/faker.providers.lorem.html\n",
        "!pip install Faker\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxWLTo3BZnSi",
        "outputId": "3326118f-eb92-4f9c-9497-c7813fe39772"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Faker\n",
            "  Downloading Faker-13.2.0-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.7/dist-packages (from Faker) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.2 in /usr/local/lib/python3.7/dist-packages (from Faker) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.4->Faker) (1.15.0)\n",
            "Installing collected packages: Faker\n",
            "Successfully installed Faker-13.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "cell_id": "00019-2d925cb0-f03e-475c-8f9f-59d9930824b3",
        "deepnote_cell_type": "code",
        "execution_millis": 104,
        "execution_start": 1606208752430,
        "output_cleared": false,
        "source_hash": "6dc5b4c8",
        "id": "DJ9bkoowLnEh"
      },
      "outputs": [],
      "source": [
        "from faker import Faker\n",
        "fake = Faker()\n",
        "Faker.seed(0)\n",
        "\n",
        "with open(\"sample.txt\",\"w\") as f:\n",
        "    f.write(fake.text(max_nb_chars=1000))\n",
        "    \n",
        "rdd_from_file = sc.textFile(\"sample.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00020-f8161a29-5442-40bc-b3ad-314ac079b925",
        "deepnote_cell_type": "markdown",
        "id": "f2aVO4LbLnEi"
      },
      "source": [
        "### Collect\n",
        "\n",
        "Action / To Driver: Return all items in the RDD to the driver in a single list\n",
        "\n",
        "![](https://github.com/pnavaro/big-data/blob/master/notebooks/images/DUO6ygB.png?raw=1)\n",
        "\n",
        "Source: https://i.imgur.com/DUO6ygB.png\n",
        "\n",
        "# Exercise \n",
        "\n",
        "Collect the text you read before from the `sample.txt`file."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(rdd_from_file.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIS75DnxaSLe",
        "outputId": "71c68b48-32fa-4373-c8dd-6ce0208c9d55"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['American whole magazine truth stop whose. On traditional measure example sense peace. Would mouth relate own chair.', 'Together range line beyond. First policy daughter need kind miss.', 'Trouble behavior style report size personal partner. During foot that course nothing draw.', 'Language ball floor meet usually board necessary. Natural sport music white.', 'Onto knowledge other his offer face country. Almost wonder employee attorney. Theory type successful together. Raise study modern miss dog Democrat quickly.', 'Every manage political record word group food break. Picture suddenly drug rule bring determine some forward. Beyond chair recently and.', 'Own available buy country store build before. Already against which continue. Look road article quickly.', 'Per structure attorney author feeling job. Mean always beyond write. Employee toward like total now.', 'Small citizen class morning. Others kind company likely.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00021-88914dd9-d7ed-435c-988d-b333620a63f9",
        "deepnote_cell_type": "markdown",
        "id": "K9pIH9BELnEi"
      },
      "source": [
        "### Map\n",
        "\n",
        "Transformation / Narrow: Return a new RDD by applying a function to each element of this RDD\n",
        "\n",
        "![](https://github.com/pnavaro/big-data/blob/master/notebooks/images/PxNJf0U.png?raw=1)\n",
        "\n",
        "Source: http://i.imgur.com/PxNJf0U.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "cell_id": "00022-e19738f1-5568-4e01-a918-31f95720f79a",
        "deepnote_cell_type": "code",
        "execution_millis": 2412,
        "execution_start": 1605183503558,
        "output_cleared": false,
        "source_hash": "8210a3cb",
        "id": "qUpRNprmLnEj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9488c441-e8e1-4143-8612-140f6b98a9b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 4, 9, 16, 25, 36, 49]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "rdd1 = sc.parallelize(list(range(8)))\n",
        "rdd1.map(lambda x: x ** 2).collect() # Square each element"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00024-0a7d639a-570b-49d3-a71a-7330ec203bc8",
        "deepnote_cell_type": "markdown",
        "id": "PJ7_If90LnEj"
      },
      "source": [
        "### Filter\n",
        "\n",
        "Transformation / Narrow: Return a new RDD containing only the elements that satisfy a predicate\n",
        "\n",
        "![](https://github.com/pnavaro/big-data/blob/master/notebooks/images/GFyji4U.png?raw=1)\n",
        "Source: http://i.imgur.com/GFyji4U.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "cell_id": "00025-9893a281-e7fd-49e4-880e-c18c223437e2",
        "deepnote_cell_type": "code",
        "id": "g2z23Hw5LnEj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49fa1869-72cf-4e0f-aab4-39b2fb528ad5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 2, 4, 6]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "# Select only the even elements\n",
        "rdd1.filter(lambda x: x % 2 == 0).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00026-c2044f27-66c2-49e4-a2c3-1551769fc2fe",
        "deepnote_cell_type": "markdown",
        "id": "_kPZ2vFtLnEj"
      },
      "source": [
        "### FlatMap\n",
        "\n",
        "Transformation / Narrow: Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results\n",
        "\n",
        "![](https://github.com/pnavaro/big-data/blob/master/notebooks/images/TsSUex8.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "cell_id": "00027-eab04ee2-2189-474e-b0ee-7cdf04d51334",
        "deepnote_cell_type": "code",
        "execution_millis": 1423,
        "execution_start": 1606209096798,
        "output_cleared": false,
        "source_hash": "ed69af29",
        "id": "VrUoxw3PLnEk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aeac2ae-a5ae-4350-b60a-a4d4c2088ea4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 100, 42, 2, 200, 42, 3, 300, 42]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "rdd2 = sc.parallelize([1,2,3])\n",
        "rdd2.flatMap(lambda x: (x, x*100, 42)).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00028-d3078029-dd5d-4852-a84f-4fb134597c5d",
        "deepnote_cell_type": "markdown",
        "id": "IrG1mpg1LnEk"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "Use FlatMap to clean the text from `sample.txt`file. Lower, remove dots and split into words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hint use \n",
        "def clean_row(line):\n",
        "    output = line.lower()\n",
        "    output = output.replace('.', ' ')\n",
        "    output = output.split(' ')\n",
        "    return output\n",
        "\n",
        "rdd_from_file.flatMap(clean_row).collect()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xX5m6SWYbSfe",
        "outputId": "5acd8513-f4ef-40de-f271-02c8b7e13caa"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['american',\n",
              " 'whole',\n",
              " 'magazine',\n",
              " 'truth',\n",
              " 'stop',\n",
              " 'whose',\n",
              " '',\n",
              " 'on',\n",
              " 'traditional',\n",
              " 'measure',\n",
              " 'example',\n",
              " 'sense',\n",
              " 'peace',\n",
              " '',\n",
              " 'would',\n",
              " 'mouth',\n",
              " 'relate',\n",
              " 'own',\n",
              " 'chair',\n",
              " '',\n",
              " 'together',\n",
              " 'range',\n",
              " 'line',\n",
              " 'beyond',\n",
              " '',\n",
              " 'first',\n",
              " 'policy',\n",
              " 'daughter',\n",
              " 'need',\n",
              " 'kind',\n",
              " 'miss',\n",
              " '',\n",
              " 'trouble',\n",
              " 'behavior',\n",
              " 'style',\n",
              " 'report',\n",
              " 'size',\n",
              " 'personal',\n",
              " 'partner',\n",
              " '',\n",
              " 'during',\n",
              " 'foot',\n",
              " 'that',\n",
              " 'course',\n",
              " 'nothing',\n",
              " 'draw',\n",
              " '',\n",
              " 'language',\n",
              " 'ball',\n",
              " 'floor',\n",
              " 'meet',\n",
              " 'usually',\n",
              " 'board',\n",
              " 'necessary',\n",
              " '',\n",
              " 'natural',\n",
              " 'sport',\n",
              " 'music',\n",
              " 'white',\n",
              " '',\n",
              " 'onto',\n",
              " 'knowledge',\n",
              " 'other',\n",
              " 'his',\n",
              " 'offer',\n",
              " 'face',\n",
              " 'country',\n",
              " '',\n",
              " 'almost',\n",
              " 'wonder',\n",
              " 'employee',\n",
              " 'attorney',\n",
              " '',\n",
              " 'theory',\n",
              " 'type',\n",
              " 'successful',\n",
              " 'together',\n",
              " '',\n",
              " 'raise',\n",
              " 'study',\n",
              " 'modern',\n",
              " 'miss',\n",
              " 'dog',\n",
              " 'democrat',\n",
              " 'quickly',\n",
              " '',\n",
              " 'every',\n",
              " 'manage',\n",
              " 'political',\n",
              " 'record',\n",
              " 'word',\n",
              " 'group',\n",
              " 'food',\n",
              " 'break',\n",
              " '',\n",
              " 'picture',\n",
              " 'suddenly',\n",
              " 'drug',\n",
              " 'rule',\n",
              " 'bring',\n",
              " 'determine',\n",
              " 'some',\n",
              " 'forward',\n",
              " '',\n",
              " 'beyond',\n",
              " 'chair',\n",
              " 'recently',\n",
              " 'and',\n",
              " '',\n",
              " 'own',\n",
              " 'available',\n",
              " 'buy',\n",
              " 'country',\n",
              " 'store',\n",
              " 'build',\n",
              " 'before',\n",
              " '',\n",
              " 'already',\n",
              " 'against',\n",
              " 'which',\n",
              " 'continue',\n",
              " '',\n",
              " 'look',\n",
              " 'road',\n",
              " 'article',\n",
              " 'quickly',\n",
              " '',\n",
              " 'per',\n",
              " 'structure',\n",
              " 'attorney',\n",
              " 'author',\n",
              " 'feeling',\n",
              " 'job',\n",
              " '',\n",
              " 'mean',\n",
              " 'always',\n",
              " 'beyond',\n",
              " 'write',\n",
              " '',\n",
              " 'employee',\n",
              " 'toward',\n",
              " 'like',\n",
              " 'total',\n",
              " 'now',\n",
              " '',\n",
              " 'small',\n",
              " 'citizen',\n",
              " 'class',\n",
              " 'morning',\n",
              " '',\n",
              " 'others',\n",
              " 'kind',\n",
              " 'company',\n",
              " 'likely',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GroupBy\n",
        "\n",
        "Transformation / Wide: Group the data in the original RDD. Create pairs where the key is the output of a user function, and the value is all items for which the function yields this key.\n",
        "\n",
        "![](https://github.com/pnavaro/big-data/blob/master/notebooks/images/gdj0Ey8.png?raw=1)"
      ],
      "metadata": {
        "id": "xxUsbkTVbX8f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "cell_id": "00029-93936a32-6fc4-4c60-b38d-7935aa2df61f",
        "deepnote_cell_type": "code",
        "id": "zcanLdM2LnEk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78d86e7c-57f8-43b0-d4b9-1feb976b86b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('J', ['John', 'James']), ('F', ['Fred']), ('A', ['Anna'])]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "rdd3 = sc.parallelize(['John', 'Fred', 'Anna', 'James'])\n",
        "rdd3 = rdd3.groupBy(lambda w: w[0])\n",
        "[(k, list(v)) for (k, v) in rdd3.collect()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00030-dd9efd67-bc72-4eb5-9e07-0a94bd233e13",
        "deepnote_cell_type": "markdown",
        "id": "-GqGo7ElLnEk"
      },
      "source": [
        "### GroupByKey\n",
        "\n",
        "Transformation / Wide: Group the values for each key in the original RDD. Create a new pair where the original key corresponds to this collected group of values.\n",
        "\n",
        "![](https://github.com/pnavaro/big-data/blob/master/notebooks/images/TlWRGr2.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "cell_id": "00031-bc1d6a4c-8103-42a9-a39a-ee8d80680447",
        "deepnote_cell_type": "code",
        "id": "18uZNUkCLnEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de87f2b4-504c-4a4f-b0cb-b673dc532fac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('B', [5, 4]), ('A', [3, 2, 1])]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "rdd4 = sc.parallelize([('B',5),('B',4),('A',3),('A',2),('A',1)])\n",
        "rdd4 = rdd4.groupByKey()\n",
        "[(j[0], list(j[1])) for j in rdd4.collect()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00032-6cff9fda-3f13-4b2b-9b2e-580ad83ecf5f",
        "deepnote_cell_type": "markdown",
        "id": "1qNLfn9uLnEl"
      },
      "source": [
        "### Join\n",
        "\n",
        "Transformation / Wide: Return a new RDD containing all pairs of elements having the same key in the original RDDs\n",
        "\n",
        "![](https://github.com/pnavaro/big-data/blob/master/notebooks/images/YXL42Nl.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "cell_id": "00033-10e3b4f2-3f6f-4a99-adac-d8bc8c92c2e9",
        "deepnote_cell_type": "code",
        "id": "_0NUSjMmLnEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0e21c6e-b46b-4150-ed87-82b1a4a74d70"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('b', (2, 5)), ('a', (1, 3)), ('a', (1, 4))]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "x = sc.parallelize([(\"a\", 1), (\"b\", 2)])\n",
        "y = sc.parallelize([(\"a\", 3), (\"a\", 4), (\"b\", 5)])\n",
        "x.join(y).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00034-7746caa4-cfd5-4206-b12d-f0fe01d65803",
        "deepnote_cell_type": "markdown",
        "id": "Qbkj3Oz6LnEm"
      },
      "source": [
        "### Distinct\n",
        "\n",
        "Transformation / Wide: Return a new RDD containing distinct items from the original RDD (omitting all duplicates)\n",
        "\n",
        "![](https://github.com/pnavaro/big-data/blob/master/notebooks/images/Vqgy2a4.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "cell_id": "00035-fd077e6b-1166-4fca-b5e8-c9a310dc59e7",
        "deepnote_cell_type": "code",
        "id": "GZ1FinH3LnEm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad56030d-6c00-4b6f-b2e4-829984fb6fac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 4, 1, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "rdd5 = sc.parallelize([1,2,3,3,4])\n",
        "rdd5.distinct().collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00036-fdbf2aa3-158f-44a2-82a3-fab846eb5ab7",
        "deepnote_cell_type": "markdown",
        "id": "hQxBGcwALnEm"
      },
      "source": [
        "### KeyBy\n",
        "\n",
        "Transformation / Narrow: Create a Pair RDD, forming one pair for each item in the original RDD. The pair’s key is calculated from the value via a user-supplied function.\n",
        "\n",
        "![](https://github.com/pnavaro/big-data/blob/master/notebooks/images/nqYhDW5.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "cell_id": "00037-e13e8ce6-ad59-4cb9-a672-336afba96873",
        "deepnote_cell_type": "code",
        "id": "1zLUbwbDLnEn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81e42fa0-37fc-42c9-b0b8-2143cfde58a1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('J', 'John'), ('F', 'Fred'), ('A', 'Anna'), ('J', 'James')]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "rdd6 = sc.parallelize(['John', 'Fred', 'Anna', 'James'])\n",
        "rdd6.keyBy(lambda w: w[0]).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00038-ee1a58a6-954d-4994-a48a-d5a98ac09051",
        "deepnote_cell_type": "markdown",
        "id": "i10IDDAgLnEn"
      },
      "source": [
        "## Actions\n",
        "\n",
        "### Map-Reduce operation \n",
        "\n",
        "Action / To Driver: Aggregate all the elements of the RDD by applying a user function pairwise to elements and partial results, and return a result to the driver\n",
        "\n",
        "![](https://github.com/pnavaro/big-data/blob/master/notebooks/images/R72uzwX.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "cell_id": "00039-cc090acd-104b-4683-844f-153af8f2956d",
        "deepnote_cell_type": "code",
        "id": "-75dk6_0LnEn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c83ca1e4-c7aa-4462-96da-8be9e2ae6e31"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "140"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "from operator import add\n",
        "rdd7 = sc.parallelize(list(range(8)))\n",
        "rdd7.map(lambda x: x ** 2).reduce(add) # reduce is an action!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00041-e22cf358-c375-48c8-ba0b-5489d848df67",
        "deepnote_cell_type": "markdown",
        "id": "9Z2VhyzmLnEn"
      },
      "source": [
        "### CountByKey\n",
        "\n",
        "Action / To Driver: Return a map of keys and counts of their occurrences in the RDD\n",
        "\n",
        "![](https://github.com/pnavaro/big-data/blob/master/notebooks/images/jvQTGv6.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "cell_id": "00042-613102f2-fea1-436a-a695-8169201af599",
        "deepnote_cell_type": "code",
        "id": "xQzsXRRKLnEn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c73a1906-c7fc-4080-b2bb-1169bd0a3cc7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int, {'A': 1, 'F': 1, 'J': 2})"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "rdd = sc.parallelize([('J', 'James'), ('F','Fred'), \n",
        "                    ('A','Anna'), ('J','John')])\n",
        "\n",
        "rdd.countByKey()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "00043-a8624fda-21b4-4d84-b41a-0b2f94951a4a",
        "deepnote_cell_type": "code",
        "id": "Wryp55wrLnEo"
      },
      "outputs": [],
      "source": [
        "# Stop the local spark cluster\n",
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00044-36e25a83-4df8-4b26-b143-ac3cf849110a",
        "deepnote_cell_type": "markdown",
        "id": "1ILIIz4RLnEo"
      },
      "source": [
        "### ExerciseWord-count in Apache Spark\n",
        "\n",
        "- Write the sample text file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00046-4fe65a61-99c3-41b6-b52c-4995e5992182",
        "deepnote_cell_type": "markdown",
        "id": "fHbK-1irLnEo"
      },
      "source": [
        "- Create the rdd with `SparkContext.textFile method`\n",
        "- lower, remove dots and split using `rdd.flatMap`\n",
        "- use `rdd.map` to create the list of key/value pair (word, 1)\n",
        "- `rdd.reduceByKey` to get all occurences\n",
        "- `rdd.takeOrdered`to get sorted frequencies of words\n",
        "\n",
        "All documentation is available [here](https://spark.apache.org/docs/2.1.0/api/python/pyspark.html?highlight=textfile#pyspark.SparkContext) for textFile and [here](https://spark.apache.org/docs/2.1.0/api/python/pyspark.html?highlight=textfile#pyspark.RDD) for RDD. \n",
        "\n",
        "For a global overview see the Transformations section of the [programming guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n"
      ]
    }
  ],
  "metadata": {
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "53ba4a86-7d9e-483d-acb5-582d7555be2b",
    "jupytext": {
      "text_representation": {
        "extension": ".md",
        "format_name": "myst",
        "format_version": "0.9",
        "jupytext_version": "1.5.2"
      }
    },
    "kernelspec": {
      "display_name": "big-data",
      "language": "python",
      "name": "big-data"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "source_map": [
      13,
      19,
      26,
      33,
      45,
      54,
      62,
      85,
      105,
      113,
      124,
      134,
      143,
      148,
      153,
      160,
      162,
      166,
      170,
      176,
      183,
      197,
      207,
      210,
      216,
      225,
      228,
      236,
      239,
      251,
      255,
      263,
      267,
      275,
      279,
      287,
      290,
      298,
      301,
      311,
      315,
      323,
      331,
      338,
      341,
      347,
      351,
      392,
      398,
      402,
      407,
      413,
      428,
      439,
      443,
      459,
      463,
      467,
      473,
      477,
      493,
      499,
      503,
      509,
      513,
      525
    ],
    "colab": {
      "name": "PySpark_Trasformation.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}